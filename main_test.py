from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import pandas as pd

# Load model & tokenizer
model_name = "openthaigpt/openthaigpt-1.0.0-70b-chat"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", torch_dtype=torch.float16)


# ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ñ‡∏≤‡∏°‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πá‡πÑ‡∏î‡πâ
def ask_law_question(question):
    messages = [
        {"role": "user", "content": f"""‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:

        {context}

        ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}"""}
    ]

    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=400, do_sample=False)
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    print("üìå ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•:\n", response)


if __name__ == "__main__":
    # Load CSV
    context = ""
    df = pd.read_csv("Extracted_Law_CSV.csv", encoding="utf-8-sig")
    for i, row in df.head(3).iterrows():  
        context += f"[{i}] {row['text']}\n\n"
        
        
    ask_law_question("‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ ‡∏°‡∏µ‡∏°‡∏≤‡∏ï‡∏£‡∏≤‡πÑ‡∏´‡∏ô‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ö‡πâ‡∏≤‡∏á")
    ask_law_question("‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏´‡∏ô‡∏°‡∏µ‡∏°‡∏≤‡∏ï‡∏£‡∏≤ ‡πí‡πí‡πñ")
