import os
import re
import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from huggingface_hub import HfFolder
import pytrec_eval

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
torch.cuda.empty_cache()

model_name = "openthaigpt/openthaigpt-1.6-72b-instruct"
token = HfFolder.get_token()

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, token=token)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=quant_config,
    device_map="auto",
    torch_dtype=torch.float16,
    token=token
)

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢
def ask_law_question(question, context):
    messages = [
        {"role": "system", "content": """ 
        ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ô‡∏±‡∏Å‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏û‡πà‡∏á‡πÅ‡∏•‡∏∞‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå
        ‡∏à‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏î‡∏µ‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î ‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ "‡πÄ‡∏•‡∏Ç‡∏°‡∏≤‡∏ï‡∏£‡∏≤‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏û‡πà‡∏á‡πÅ‡∏•‡∏∞‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå" ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î 5 ‡∏°‡∏≤‡∏ï‡∏£‡∏≤‡πÅ‡∏£‡∏Å ‡∏ó‡∏µ‡πà‡∏á‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÄ‡πÄ‡∏•‡∏∞‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≠‡∏° ‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢ ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏Å‡∏¥‡∏ô 5 ‡∏°‡∏≤‡∏ï‡∏£‡∏≤ ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏°‡∏≤‡∏ï‡∏£‡∏≤"
        ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏à‡∏∏‡∏•‡∏†‡∏≤‡∏Ñ ‡πÄ‡∏ä‡πà‡∏ô: 1336, 1299, 1520, 1500, 1337
        """},
        {"role": "user", "content": f"""
        ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°:\n{context}\n\n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:\n{question}
        """},
    ]


    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=512,
        do_sample=False,
        eos_token_id=tokenizer.eos_token_id,
    )

    generated_ids = [
        output_ids[len(input_ids):] 
        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response.strip()

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏¢‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏•‡∏Ç‡∏°‡∏≤‡∏ï‡∏£‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô 1336
def extract_law_numbers(text):
    return re.findall(r'\d+', text)

def evaluate_with_pytrec(df, k=5):
    qrel = {}
    run = {}

    for idx, row in df.iterrows():
        qid = f"q{idx}"
        true_ids = extract_law_numbers(str(row['answers']))
        pred_ids = extract_law_numbers(str(row['predicted_law']))[:k]

        qrel[qid] = {law_id: 1 for law_id in true_ids}
        run[qid] = {law_id: 1.0 / (i + 1) for i, law_id in enumerate(pred_ids)}

    evaluator = pytrec_eval.RelevanceEvaluator(qrel, {"recip_rank", f"recall_{k}"})
    results = evaluator.evaluate(run)

    df["recall_at_k"] = 0.0
    df["mrr_at_k"] = 0.0
    recall_scores = []
    mrr_scores = []

    for qid, metrics in results.items():
        idx = int(qid[1:])
        recall = metrics.get(f"recall_{k}", 0.0)
        mrr = metrics.get("recip_rank", 0.0)

        df.at[idx, "recall_at_k"] = recall
        df.at[idx, "mrr_at_k"] = mrr

        recall_scores.append(recall)
        mrr_scores.append(mrr)

    avg_recall = sum(recall_scores) / len(recall_scores) if recall_scores else 0.0
    avg_mrr = sum(mrr_scores) / len(mrr_scores) if mrr_scores else 0.0

    return avg_recall, avg_mrr

# Main process
if __name__ == "__main__":
    df = pd.read_csv("/workspace/test/data_case_100.csv", encoding="utf-8-sig")

    if "text" not in df.columns or "answers" not in df.columns:
        raise ValueError("CSV ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ column 'text' ‡πÅ‡∏•‡∏∞ 'answers'")

    df['predicted_law'] = ""

    for i in range(df.shape[0]):
        context = df['text'].iloc[i]

        full_prompt = f"""
        ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏î‡∏µ‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏°‡∏≤‡∏ï‡∏£‡∏≤‡πÅ‡∏û‡πà‡∏á‡πÅ‡∏•‡∏∞‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î 5 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á 
        """.strip()

        ans = ask_law_question(full_prompt, context)
        df.at[i, 'predicted_law'] = ans
        print(f"[{i}] ‚úÖ Answer: {ans}")

    df_eval = df
    recall_k, mrr_k = evaluate_with_pytrec(df_eval, k=5)

    print(f"\nüéØ Recall@5 (‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏à‡∏≤‡∏Å {len(df_eval)} ‡πÅ‡∏ñ‡∏ß): {recall_k:.4f}")
    print(f"üìà MRR@5 (‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏à‡∏≤‡∏Å {len(df_eval)} ‡πÅ‡∏ñ‡∏ß): {mrr_k:.4f}")

    df_eval.to_csv("/workspace/test/output_predicted.csv", index=False, encoding="utf-8-sig")
