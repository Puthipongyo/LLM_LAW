import os
import re
import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from huggingface_hub import HfFolder

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
torch.cuda.empty_cache()

model_name = "scb10x/llama3.1-typhoon2-70b-instruct"
token = HfFolder.get_token()

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, token=token)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=quant_config,
    device_map="auto",
    torch_dtype=torch.float16,
    token=token
)

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢
def ask_law_question(question, context):
    messages = [
        {"role": "system", "content": """ 
        ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ô‡∏±‡∏Å‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏û‡πà‡∏á‡πÅ‡∏•‡∏∞‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå
        ‡∏à‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏î‡∏µ‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î ‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ "‡πÄ‡∏•‡∏Ç‡∏°‡∏≤‡∏ï‡∏£‡∏≤‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏û‡πà‡∏á‡πÅ‡∏•‡∏∞‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå" ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î 5 ‡∏°‡∏≤‡∏ï‡∏£‡∏≤‡πÅ‡∏£‡∏Å ‡∏ó‡∏µ‡πà‡∏á‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÄ‡πÄ‡∏•‡∏∞‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≠‡∏° ‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢ ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏Å‡∏¥‡∏ô 5 ‡∏°‡∏≤‡∏ï‡∏£‡∏≤ ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏°‡∏≤‡∏ï‡∏£‡∏≤"
        ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏à‡∏∏‡∏•‡∏†‡∏≤‡∏Ñ ‡πÄ‡∏ä‡πà‡∏ô: 1336, 1299, 1520, 1500, 1337
        """},
        {"role": "user", "content": f"""
        ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°:\n{context}\n\n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:\n{question}
        """},
    ]

    input_ids = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(model.device)

    terminators = [
        tokenizer.eos_token_id,
        tokenizer.convert_tokens_to_ids("<|eot_id|>")
    ]

    outputs = model.generate(
        input_ids,
        max_new_tokens=256,
        eos_token_id=terminators,
        do_sample=False,
        temperature=0.7,
        top_p=0.95,
    )
    response = outputs[0][input_ids.shape[-1]:]
    decoded = tokenizer.decode(response, skip_special_tokens=True)

    return decoded.strip()

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏¢‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏•‡∏Ç‡∏°‡∏≤‡∏ï‡∏£‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô 1336
def extract_law_numbers(text):
    return re.findall(r'\d+', text)

def evaluate_mrr_at_k(df, k=5):
    df['recall_at_k'] = 0.0
    df['mrr_at_k'] = 0.0
    mrr_scores = []
    recall_scores = []

    for i in range(df.shape[0]):
        true_ids = extract_law_numbers(str(df.at[i, 'answers']))
        pred_ids = extract_law_numbers(str(df.at[i, 'predicted_law']))[:k]

        # ‡∏´‡∏≤‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡πÉ‡∏ô top-k
        rank = None
        for idx, pid in enumerate(pred_ids):
            if pid in true_ids:
                rank = idx + 1  # index ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 0
                break

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì MRR@K
        rr = 1.0 / rank if rank is not None else 0.0
        recall = len(set(true_ids) & set(pred_ids)) / len(true_ids) if true_ids else 0.0

        df.at[i, 'mrr_at_k'] = rr
        df.at[i, 'recall_at_k'] = recall

        mrr_scores.append(rr)
        recall_scores.append(recall)

    avg_mrr = sum(mrr_scores) / len(mrr_scores) if mrr_scores else 0.0
    avg_recall = sum(recall_scores) / len(recall_scores) if recall_scores else 0.0

    return avg_recall, avg_mrr

# Main process
if __name__ == "__main__":
    df = pd.read_csv("/workspace/test/data_case_100.csv", encoding="utf-8-sig")

    if "text" not in df.columns or "answers" not in df.columns:
        raise ValueError("CSV ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ column 'text' ‡πÅ‡∏•‡∏∞ 'answers'")

    df['predicted_law'] = ""

    for i in range(df.shape[0]):
        context = df['text'].iloc[i]

        full_prompt = f"""
        ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏î‡∏µ‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏°‡∏≤‡∏ï‡∏£‡∏≤‡πÅ‡∏û‡πà‡∏á‡πÅ‡∏•‡∏∞‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î 5 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á 
        """.strip()

        ans = ask_law_question(full_prompt, context)
        df.at[i, 'predicted_law'] = ans
        print(f"[{i}] ‚úÖ Answer: {ans}")

        if i == 50:
            break

    df_eval = df.head(50)
    recall_k, mrr_k = evaluate_mrr_at_k(df_eval, k=5)

    print(f"\nüéØ Recall@5 (‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏à‡∏≤‡∏Å {len(df_eval)} ‡πÅ‡∏ñ‡∏ß): {recall_k:.4f}")
    print(f"üìà MRR@5 (‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏à‡∏≤‡∏Å {len(df_eval)} ‡πÅ‡∏ñ‡∏ß): {mrr_k:.4f}")

    df_eval.to_csv("/workspace/test/output_predicted.csv", index=False, encoding="utf-8-sig")
